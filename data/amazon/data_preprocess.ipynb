{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Run this code below Kavach, if in the inference I  get No Title, or anything weird.\n",
    "The only reason for getting weird outputs in the .txt file is because of the wrong files being fed.\n",
    "So if the pipeline is getting weird.\n",
    "Run this code first, then directly go start doing the stage-1 and stage-2 and inference.\n",
    "\n",
    "Also, for llms with 3b, 1b, 7b I have to run it from starting.\n",
    "\n",
    "\n",
    "But Once I have ran stage 1 for  7b, then only run stage -2 and inference for other 7 b.\n",
    "\n",
    "\n",
    "Tomorrow also test 14b, if it could be run"
   ],
   "id": "36e0c5f7630b417b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# This one seems to be working correctly for now.\n",
    "#Change the path below, and put it int the sasrec-dataprecesss\n",
    "# Put the path of amazon\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "import gzip\n",
    "import json\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "def parse(path):\n",
    "    g = gzip.open(path, 'rb')\n",
    "    for l in tqdm(g):\n",
    "        yield json.loads(l)\n",
    "\n",
    "def preprocess(fname):\n",
    "    countU = defaultdict(lambda: 0)\n",
    "    countP = defaultdict(lambda: 0)\n",
    "    line = 0\n",
    "\n",
    "    # Set the absolute base directory where your files are located.\n",
    "    base_dir = \"/home/kavach/Dev/Publication/A-LLM-Rec/A-LLMRec_copy_original/temp/Llama\"\n",
    "\n",
    "    # Build the absolute file path for the review data file.\n",
    "    file_path = os.path.join(base_dir, f'{fname}.json.gz')\n",
    "\n",
    "    # First pass: counting interactions for each user and item.\n",
    "    for l in parse(file_path):\n",
    "        line += 1\n",
    "        asin = l['asin']\n",
    "        rev = l['reviewerID']\n",
    "        time = l['unixReviewTime']\n",
    "        countU[rev] += 1\n",
    "        countP[asin] += 1\n",
    "\n",
    "    usermap = {}\n",
    "    usernum = 0\n",
    "    itemmap = {}  # maps ASIN to an initial (possibly non-sequential) ID\n",
    "    itemnum = 0\n",
    "    User = {}\n",
    "    review_dict = {}\n",
    "    name_dict = {'title': {}, 'description': {}}\n",
    "\n",
    "    # Build the absolute path for the meta file.\n",
    "    meta_file_path = os.path.join(base_dir, f'meta_{fname}.json')\n",
    "    with open(meta_file_path, 'r') as f:\n",
    "        json_data = f.readlines()\n",
    "    data_list = [json.loads(line.strip()) for line in json_data]\n",
    "    meta_dict = {}\n",
    "    for entry in data_list:\n",
    "        meta_dict[entry['asin']] = entry\n",
    "\n",
    "    print(f\"Loaded meta_dict with {len(meta_dict)} entries for dataset {fname}\")\n",
    "\n",
    "    # Create sets to track unique ASINs before and after filtering for valid meta data.\n",
    "    all_valid_asins = set()\n",
    "    filtered_valid_asins = set()\n",
    "\n",
    "    # Second pass: process the review data and build mappings.\n",
    "    for l in parse(file_path):\n",
    "        asin = l['asin']\n",
    "        rev = l['reviewerID']\n",
    "        time = l['unixReviewTime']\n",
    "\n",
    "        # Set threshold for filtering interactions.\n",
    "        threshold = 5\n",
    "        if ('Beauty' in fname) or ('Toys' in fname) or ('Magazine_Subscriptions' in fname):\n",
    "            threshold = 3\n",
    "\n",
    "        if countU[rev] < threshold or countP[asin] < threshold:\n",
    "            continue\n",
    "\n",
    "        # Add the ASIN to the set of items passing the threshold.\n",
    "        all_valid_asins.add(asin)\n",
    "\n",
    "        # Option B: Filter out reviews with missing meta data.\n",
    "        if asin not in meta_dict:\n",
    "            print(f\"Warning: Skipping review for ASIN {asin} due to missing meta data.\")\n",
    "            continue\n",
    "\n",
    "        # Only if meta data exists, add the ASIN to the filtered set.\n",
    "        filtered_valid_asins.add(asin)\n",
    "\n",
    "        # Map reviewer to a new integer ID.\n",
    "        if rev in usermap:\n",
    "            userid = usermap[rev]\n",
    "        else:\n",
    "            usernum += 1\n",
    "            userid = usernum\n",
    "            usermap[rev] = userid\n",
    "            User[userid] = []\n",
    "\n",
    "        # Map product ASIN to an initial item ID.\n",
    "        if asin in itemmap:\n",
    "            itemid = itemmap[asin]\n",
    "        else:\n",
    "            itemnum += 1\n",
    "            itemid = itemnum\n",
    "            itemmap[asin] = itemid\n",
    "        User[userid].append([time, itemid])\n",
    "\n",
    "        # Build review_dict (optional) and name_dict with meta data.\n",
    "        if itemmap[asin] in review_dict:\n",
    "            try:\n",
    "                review_dict[itemmap[asin]]['review'][usermap[rev]] = l['reviewText']\n",
    "            except Exception as e:\n",
    "                pass\n",
    "            try:\n",
    "                review_dict[itemmap[asin]]['summary'][usermap[rev]] = l['summary']\n",
    "            except Exception as e:\n",
    "                pass\n",
    "        else:\n",
    "            review_dict[itemmap[asin]] = {'review': {}, 'summary': {}}\n",
    "            try:\n",
    "                review_dict[itemmap[asin]]['review'][usermap[rev]] = l['reviewText']\n",
    "            except Exception as e:\n",
    "                pass\n",
    "            try:\n",
    "                review_dict[itemmap[asin]]['summary'][usermap[rev]] = l['summary']\n",
    "            except Exception as e:\n",
    "                pass\n",
    "\n",
    "        # Since we know the meta data exists, add it directly.\n",
    "        if len(meta_dict[asin].get('description', '')) == 0:\n",
    "            name_dict['description'][itemmap[asin]] = 'Empty description'\n",
    "        else:\n",
    "            # Assuming description is a list; use the first element.\n",
    "            name_dict['description'][itemmap[asin]] = meta_dict[asin]['description'][0]\n",
    "        name_dict['title'][itemmap[asin]] = meta_dict[asin]['title']\n",
    "\n",
    "    # Print counts before and after filtering.\n",
    "    print(f\"Total unique items that pass threshold (before meta check): {len(all_valid_asins)}\")\n",
    "    print(f\"Unique items with valid meta data (after filtering): {len(filtered_valid_asins)}\")\n",
    "    missing_count = len(all_valid_asins) - len(filtered_valid_asins)\n",
    "    print(f\"Unique items missing meta data: {missing_count}\")\n",
    "    print(f\"Number of items with meta data before re-indexing: {len(name_dict['title'])}\")\n",
    "\n",
    "    # --- Re-indexing step ---\n",
    "    new_title = {}\n",
    "    new_description = {}\n",
    "    new_itemmap = {}  # maps old item id to new sequential id\n",
    "    new_index = 1\n",
    "    # Sort keys for reproducibility.\n",
    "    for old_key in sorted(name_dict['title'].keys()):\n",
    "        new_itemmap[old_key] = new_index\n",
    "        new_title[new_index] = name_dict['title'][old_key]\n",
    "        new_description[new_index] = name_dict['description'][old_key]\n",
    "        new_index += 1\n",
    "    # Replace name_dict with re-indexed dictionaries.\n",
    "    name_dict['title'] = new_title\n",
    "    name_dict['description'] = new_description\n",
    "    new_itemnum = len(new_title)\n",
    "\n",
    "    # Update the User interactions: remap item IDs using new_itemmap.\n",
    "    for userid in User.keys():\n",
    "        for i in range(len(User[userid])):\n",
    "            old_itemid = User[userid][i][1]\n",
    "            if old_itemid in new_itemmap:\n",
    "                User[userid][i][1] = new_itemmap[old_itemid]\n",
    "        User[userid].sort(key=lambda x: x[0])\n",
    "\n",
    "    # Save the re-indexed meta data file.\n",
    "    output_path = os.path.join(base_dir, f'{fname}_text_name_dict.json.gz')\n",
    "    with open(output_path, 'wb') as tf:\n",
    "        pickle.dump(name_dict, tf)\n",
    "\n",
    "    # Write user-item interaction pairs using the new indexing.\n",
    "    txt_output = os.path.join(base_dir, f'{fname}.txt')\n",
    "    with open(txt_output, 'w') as f:\n",
    "        for user in User.keys():\n",
    "            for i in User[user]:\n",
    "                f.write('%d %d\\n' % (user, i[1]))\n",
    "\n",
    "    print(\"User count:\", usernum, \"Original item count:\", itemnum, \"Re-indexed item count:\", new_itemnum)\n",
    "\n"
   ],
   "id": "c52b45513e50cf64"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
