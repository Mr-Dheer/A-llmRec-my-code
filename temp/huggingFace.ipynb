{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-06T12:50:13.359799Z",
     "start_time": "2025-02-06T12:49:06.766262Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from huggingface_hub import QuestionAnsweringInput\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM\n",
    "from transformers.utils import logging\n",
    "logging.get_logger().setLevel(\"INFO\")\n",
    "\n",
    "model = 'deepseek-ai/DeepSeek-R1-Distill-Qwen-7B'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "model = AutoModelForCausalLM.from_pretrained(model)\n"
   ],
   "id": "c3755b3f50264b5e",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kavach/Dev/anaconda3/envs/ALLM-Rec/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "loading file tokenizer.model from cache at None\n",
      "loading file tokenizer.json from cache at /home/kavach/.cache/huggingface/hub/models--deepseek-ai--DeepSeek-R1-Distill-Qwen-7B/snapshots/14dd1130311655b43c3ce41dd505f70f6ca89845/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /home/kavach/.cache/huggingface/hub/models--deepseek-ai--DeepSeek-R1-Distill-Qwen-7B/snapshots/14dd1130311655b43c3ce41dd505f70f6ca89845/tokenizer_config.json\n",
      "loading file chat_template.jinja from cache at None\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "loading configuration file config.json from cache at /home/kavach/.cache/huggingface/hub/models--deepseek-ai--DeepSeek-R1-Distill-Qwen-7B/snapshots/14dd1130311655b43c3ce41dd505f70f6ca89845/config.json\n",
      "Model config Qwen2Config {\n",
      "  \"_name_or_path\": \"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\",\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151643,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3584,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 18944,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"max_window_layers\": 28,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 28,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 4,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.48.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_mrope\": false,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 152064\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at /home/kavach/.cache/huggingface/hub/models--deepseek-ai--DeepSeek-R1-Distill-Qwen-7B/snapshots/14dd1130311655b43c3ce41dd505f70f6ca89845/model.safetensors.index.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151643\n",
      "}\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [01:03<00:00, 31.52s/it]\n",
      "All model checkpoint weights were used when initializing Qwen2ForCausalLM.\n",
      "\n",
      "All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at deepseek-ai/DeepSeek-R1-Distill-Qwen-7B.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.\n",
      "loading configuration file generation_config.json from cache at /home/kavach/.cache/huggingface/hub/models--deepseek-ai--DeepSeek-R1-Distill-Qwen-7B/snapshots/14dd1130311655b43c3ce41dd505f70f6ca89845/generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151646,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": 151643,\n",
      "  \"temperature\": 0.6,\n",
      "  \"top_p\": 0.95\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-06T12:23:48.896984Z",
     "start_time": "2025-02-06T12:21:38.225253Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Gernerate text\n",
    "\n",
    "inputText = 'What upon a Time, in India'\n",
    "input_ids=tokenizer(inputText, return_tensors=\"pt\").input_ids\n",
    "\n",
    "output_ids = model.generate(input_ids, max_length=100)\n",
    "output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(output_text)"
   ],
   "id": "a167c011c1aa3b49",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What upon a Time, in India, a mathematician had a method to solve equations, but that method was not known to the Western World until 999 years later. The mathematician was a contemporary of Ramanujan, and his work was considered to be a significant contribution to the field of mathematics, but it was unknown in the West until much later. The mathematician's name is 7 letters long, starts with a 'C', and ends with a 'C'.\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-06T12:51:05.419987Z",
     "start_time": "2025-02-06T12:50:27.034568Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Question Answering\n",
    "context ='''\n",
    "The Eiffel Tower is a wrought-iron lattice tower located on the Champ de Mars in Paris, France.\n",
    "It was designed by Gustave Eiffel and completed in 1889. The tower is 330 meters tall and is one of the most recognizable structures in the world.\n",
    "'''\n",
    "question = 'Who designed the Eiffel Tower ?'\n",
    "inputText = f'Context:{context}\\n\\nQuestion:{question}\\n\\nAnswer: '\n",
    "\n",
    "# Tokenise Input\n",
    "input_ids= tokenizer(inputText, return_tensors='pt').input_ids\n",
    "\n",
    "# Tokenise Output\n",
    "output_ids = model.generate(input_ids, max_length=100)\n",
    "output_text= tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print('Generated Answer: ', output_text)\n",
    "\n"
   ],
   "id": "83b54f2058d40732",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Answer:  Context:\n",
      "The Eiffel Tower is a wrought-iron lattice tower located on the Champ de Mars in Paris, France.\n",
      "It was designed by Gustave Eiffel and completed in 1889. The tower is 330 meters tall and is one of the most recognizable structures in the world.\n",
      "\n",
      "\n",
      "Question:Who designed the Eiffel Tower ?\n",
      "\n",
      "Answer:  Gustave Eiffel\n",
      "\n",
      "But, I need to write a sentence that includes both the question and the answer\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-06T13:08:09.121595Z",
     "start_time": "2025-02-06T12:59:57.655724Z"
    }
   },
   "cell_type": "code",
   "source": [
    "question = 'Who built  Qutub Minar ?'\n",
    "inputText = f'Question:{question}\\n\\nAnswer:'\n",
    "\n",
    "input_ids= tokenizer(inputText, return_tensors='pt').input_ids\n",
    "\n",
    "output_ids = model.generate(input_ids, max_length=1000)\n",
    "output_text= tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print('Generated Answer: ', output_text)\n"
   ],
   "id": "17880e161b008cd1",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Answer:  Question:Who built  Qutub Minar ?\n",
      "\n",
      "Answer: The Qutub Minar was built by the Mughal Emperor Suleiman the Magnificent.\n",
      "\n",
      "But I need to find out the exact date when it was constructed.\n",
      "\n",
      "I remember it was built around the 16th century, but I'm not sure if it's 1590, 1594, or 1600.\n",
      "\n",
      "I think it's 1594, but I'm not certain.\n",
      "\n",
      "Maybe I should check a reliable source or use some clues from the structure or other historical records.\n",
      "\n",
      "Wait, the Mughal Empire was active from the 16th to the 18th century, so 1594 falls within that period.\n",
      "\n",
      "I've heard that the Mughal Emperor Suleiman the Magnificent was known for his architectural achievements, so he might have been the builder.\n",
      "\n",
      "But I need to confirm the exact year.\n",
      "\n",
      "Perhaps I can recall any specific events or anniversaries related to the construction of Qutub Minar.\n",
      "\n",
      "Alternatively, I can think about other Mughal buildings and their construction timelines to estimate.\n",
      "\n",
      "I think the Qutub Minar was built during the reign of Suleiman I, who ruled from 1587 to 1600.\n",
      "\n",
      "If he started construction around 1590, it might have been completed by 1594.\n",
      "\n",
      "I'm leaning towards 1594 as the year it was built.\n",
      "\n",
      "But I'm not entirely sure, so I should verify this information.\n",
      "</think>\n",
      "\n",
      "The Qutub Minar was built by the Mughal Emperor Suleiman the Magnificent, and it was constructed in the year 1594.\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-06T13:37:45.572107Z",
     "start_time": "2025-02-06T13:34:14.964425Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Story Generation\n",
    "def generate_story(prompt):\n",
    "    # Tokenize the prompt (user's input)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    input_ids = inputs['input_ids']\n",
    "    attention_mask = inputs['attention_mask']\n",
    "\n",
    "    # Generate text based on the input prompt\n",
    "    output_ids = model.generate(input_ids, attention_mask=attention_mask, max_length=150, temperature=0.7, top_p=0.9)\n",
    "\n",
    "    # Decode the generated text back to human-readable text\n",
    "    output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    # Return the generated text (story or dialogue)\n",
    "    return output_text\n",
    "\n",
    "# Example usage: Prompt for generating a creative story\n",
    "print(generate_story(\"Once upon a time, in a world full of mystery\"))"
   ],
   "id": "6f375edfbd5755c1",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, in a world full of mystery, there are two sets of numbers: set \\( A \\) and set \\( B \\). Set \\( A \\) contains the numbers \\( 9999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-06T13:27:52.183082Z",
     "start_time": "2025-02-06T13:24:50.082366Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def summarize_text(text):\n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "\n",
    "    # Generate the summary using the model\n",
    "    summary_ids = model.generate(inputs['input_ids'], max_length=200, min_length=50, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
    "\n",
    "    # Decode the summary and return it\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    return summary\n",
    "\n",
    "# Example usage: Summarizing a long document\n",
    "document = \"\"\"\n",
    "In the rapidly evolving world of artificial intelligence, the need for advanced algorithms that can\n",
    "process vast amounts of data has never been greater. The advent of neural networks and deep learning\n",
    "has paved the way for significant breakthroughs in fields such as natural language processing, computer vision,\n",
    "and robotics. These systems have been able to outperform traditional methods in many areas, but challenges\n",
    "remain in terms of interpretability, ethics, and ensuring that AI systems are used in ways that benefit society.\n",
    "\"\"\"\n",
    "summary = summarize_text(document)\n",
    "print(\"Summary:\", summary)"
   ],
   "id": "738068a5a6f6aaa1",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary: \n",
      "In the rapidly evolving world of artificial intelligence, the need for advanced algorithms that can\n",
      "process vast amounts of data has never been greater. The advent of neural networks and deep learning\n",
      "has paved the way for significant breakthroughs in fields such as natural language processing, computer vision,\n",
      "and robotics. These systems have been able to outperform traditional methods in many areas, but challenges\n",
      "remain in terms of interpretability, ethics, and ensuring that AI systems are used in ways that benefit society.\n",
      "As AI continues to advance, it's crucial to address these challenges to ensure that the technology can be\n",
      "applied effectively and responsibly in the real world.\n",
      "\n",
      "Let me know if you'd like me to expand on any part of this prompt, or if you'd like me to write a different type of response.\n",
      "\n",
      "Okay, so I need to write a response to this prompt about the challenges in AI, specifically focusing on interpretability, ethics, and societal impact. I should make sure to elaborate on each of\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-07T14:34:38.767319Z",
     "start_time": "2025-02-07T14:34:38.131778Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ],
   "id": "7c87000baed740ab",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-07T14:36:49.695447Z",
     "start_time": "2025-02-07T14:36:49.693621Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\""
   ],
   "id": "16a5e5fd34be9a94",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# masti",
   "id": "10f431d26fa8974b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 5,
 "nbformat_minor": 9
}
